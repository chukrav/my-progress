(venv) root@16cf8fd931f5:/home/workspace# cat README.md
# Loading Pre-Trained Models

In this exercise, you'll work to download and load a few of the pre-trained models avail
able 
in the OpenVINO toolkit.

First, you can navigate to the [Pre-Trained Models list](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models) in a separate window or tab, as well as the page that gives all of the model names [here](https://docs.openvinotoolkit.org/latest/_models_intel_index.html).

Your task here is to download the below three pre-trained models using the Model Downloader tool, as detailed on the same page as the different model names. Note that you *do not need to download all of the available pre-trained models* - doing so would cause your workspace to crash, as the workspace will limit you to 3 GB of downloaded models.

### Task 1 - Find the Right Models
Using the [Pre-Trained Model list](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models), determine which models could accomplish the following tasks (there may be some room here in determining which model to download):
- Human Pose Estimation
- Text Detection
- Determining Car Type & Color

### Task 2 - Download the Models
Once you have determined which model best relates to the above tasks, use the Model Downloader tool to download them into the workspace for the following precision levels:
- Human Pose Estimation: All precision levels
- Text Detection: FP16 only
- Determining Car Type & Color: INT8 only

**Note**: When downloading the models in the workspace, add the `-o` argument (along with any other necessary arguments) with `/home/workspace` as the output directory. The default download directory will not allow the files to be written there within the workspace, as it is a read-only directory.

### Task 3 - Verify the Downloads
You can verify the download of these models by navigating to: `/home/workspace/intel` (if you followed the above note), and checking whether a directory was created for each of the three models, with included subdirectories for each precision, with respective `.bin` and `.xml` for each model.

**Hint**: Use the `-h` command with the Model Downloader tool if you need to check out the possible arguments to include when downloading specific models and precisions.
(venv) root@16cf8fd931f5:/home/workspace# cat solution/README.md
# Loading Pre-Trained Models - Solution

In this exercise, you downloaded and loaded a few of the pre-trained models available 
in the OpenVINO toolkit.

- There's certainly some wiggle room on which pre-trained models to use for each, but I just used the primary model linked to from the [Pre-Trained Model list](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models) for each:
  - Human Pose Estimation: [human-pose-estimation-0001](https://docs.openvinotoolkit.org/latest/_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html)
  - Text Detection: [text-detection-0004](http://docs.openvinotoolkit.org/latest/_models_intel_text_detection_0004_description_text_detection_0004.html)
  - Determining Car Type & Color: [vehicle-attributes-recognition-barrier-0039](https://docs.openvinotoolkit.org/latest/_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html)
- To access the Model Downloader, you can `cd /opt/intel/openvino/deployment_tools/tools/model_downloader`
- From there, if you use `sudo ./downloader.py -h`, you can see the various arguments you can feed to the downloader. I noted `--name` and `--precisions`  as useful arguments to feed both the model name and which precision(s) I wanted to download.
- The format therefore is:
  - `sudo ./downloader.py --name {model-name} {--precisions {precision-levels}}`, e.g. `sudo ./downloader.py --name human-pose-estimation-0001` for all precision levels, or `sudo ./downloader.py --name text-detection-0004 --precisions FP16` for just FP16 precision, for example.
  - In the workspace, you should add a `-o` argument at the end, e.g.  `sudo ./downloader.py --name human-pose-estimation-0001 -o /home/workspace`
- I can then check the `/home/workspace/intel` directory and see that each model is in that directory, with the desired precisions included, and `.bin` and `.xml` files for each model precision. Note on a local machine this defaults to `/opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/intel`
- Some of these directory paths are pretty long - you may want to consider adding symbolic links to these directories for faster use.
(venv) root@16cf8fd931f5:/home/workspace# 


use downloader with  -o /home/workspace option to successed.

Tools described in Lesson: 1.08 Relevant Tools and Prerequisites
OpenVINO™ toolkit Documentation:
https://docs.openvinotoolkit.org/latest/index.html
Code Repository:
https://github.com/opencv/dldt.git
Intel® Neural Compute Stick 2 and Open Source OpenVINO™ Toolkit
https://software.intel.com/en-us/articles/intel-neural-compute-stick-2-and-open-source-openvino-toolkit

Work version of OpenVINO™ toolkit: -------------------
 You have signed up for the Intel® Distribution of OpenVINO™ toolkit for Windows*. 
 You will receive an email with the serial number listed below and the download location
 for future reference.

Serial number : C9PJ-F8476P38
    Save this serial number. You may need it to activate your product in the installer.
    For your reference, you will receive an email that includes your serial number and download
    instructions.
--------------------------------------------------------
Course Structure L1.6:

    1. focus on AI at the Edge using the Intel® Distribution of OpenVINO™ Toolkit.
    2. start off with pre-trained models in the OpenVINO™ Open Model Zoo.
       Even without training, you can deploy models created for many applications.
    3. learn about the Model Optimizer, which can take a model you trained in TensorFlow,
       PyTorch, Caffe and more, create an Intermediate Representation (IR) optimized for
       inference with OpenVINO™ and Intel® hardware.
    4. learn about the Inference Engine, where the actual inference is performed on the IR model.
    5. Lastly, we'll hit some more topics on deploying at the edge, including things like handling
    input streams, processing model outputs, and the lightweight MQTT architecture used to publish
    data from your edge models to the web.
    
Lesson 1:   Recap Summary

we covered:

    The basics of the edge
    The importance of the edge and its history
    Edge applications
    The structure of the course
        Pre-Trained Models
        The Model Optimizer
        The Inference Engine
        More edge topics (MQTT, servers, etc.)
    An overview of the project


Lesson 2:
The OpenVINO™ Toolkit’s name comes from “Open Visual Inferencing and Neural Network .
We covered three types of computer vision models in the video: Classification, Detection,
and Segmentation. 

Further Research

Getting used to reading research papers is a key skill to build when working with AI and Computer Vision.
 Below, you can find the original research papers on some of the networks we discussed in this section.

    SSD: https://arxiv.org/abs/1512.02325
    YOLO: https://arxiv.org/abs/1506.02640
    Faster RCNN: https://arxiv.org/abs/1506.01497
    MobileNet: https://arxiv.org/abs/1704.04861
    ResNet: https://arxiv.org/abs/1512.03385
    Inception: https://arxiv.org/pdf/1409.4842.pdf





Intermediate Representations (IR)
Further Research
You can find the main developer documentation on converting models in the OpenVINO™ Toolkit here.
https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html
 We’ll cover how to do so with TensorFlow, Caffe and ONNX (useful for PyTorch) over the next several pages.
You can find the documentation on different layer names when converted to an IR here.
https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html
Finally, you can find more in-depth data on each of the Intermediate Representation layers themselves here.
https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_IRLayersCatalogSpec.html

----------------------------------------------------------
We used to put a long pathes in our commands , and
 being suggested considering setting a path environment variable for the Model Optimizer
 if we are working locally on a Linux-based machine. You could do something like this:

`export MOD_OPT=/opt/intel/openvino/deployment_tools/model_optimizer`
                /opt/intel/openvino/deployment_tools/model_optimizer/
				/opt/intel/openvino/deployment_tools/model_optimizer/

And then when you need to use it, you can utilize it with $MOD_OPT/mo.py instead of entering
 the full long path each time. In this case, that would also help shorten
 the path to the ssd_v2_support.json file used.
 --------------------------------------------------------
 alexei.json
 Leshakr1459$
 You have signed up for the Intel® Distribution of OpenVINO™ toolkit for Windows*. 
 You will receive an email with the serial number listed below and the download location
 for future reference.

Serial number : C9PJ-55TJCBKT

    Save this serial number. You may need it to activate your product in the installer.
    For your reference, you will receive an email that includes your serial number and download
    instructions.
    Your Serial Number

    Your serial number for this product is C9PJ-55TJCBKT. Save this number, since you will need 
    it to install the software.

Ex_ConvertCaffeModel =====================
MOD_OPT <- /opt/intel/openvino/deployment_tools/model_optimizer/mo.py 

$MOD_OPT/mo.py --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt

wget <link> - to download file !!!!!
/home/workspace# 
ssd_mobilenet_v2_coco_2018_03_29

use minGW shell : tar is working at least!
`tar -xvf ssd_mobilenet_v2_coco_2018_03_29.tar.gz`
# /c/Users/Lesha/Udacity/DLearning/IntelEdge/Ex_ConvertTFModel

Exercise: Convert an ONNX Model ==================
$MOD_OPT/mo.py --input_model model.onnx
Output:
[ SUCCESS ] Generated IR model.
[ SUCCESS ] XML file: /home/workspace/bvlc_alexnet/./model.xml
[ SUCCESS ] BIN file: /home/workspace/bvlc_alexnet/./model.bin
[ SUCCESS ] Total execution time: 5.05 seconds. 

Cutting a model is mostly applicable for TensorFlow models. As we saw earlier in converting these models, they sometimes have some extra complexities. Some common reasons for cutting are:

    The model has pre- or post-processing parts that don’t translate to existing Inference Engine layers.
    The model has a training part that is convenient to be kept in the model, but is not used during inference.
    The model is too complex with many unsupported operations, so the complete model cannot be converted in one shot.
    The model is one of the supported SSD models. In this case, you need to cut a post-processing part off.
    There could be a problem with model conversion in the Model Optimizer or with inference in the Inference Engine. To localize the issue, cutting the model could help to find the problem

Lesson 3 Recap:

Ex_ :

    TensorFlow Model Optimizer extractor extension:
        $CLWS/cl_cosh/user_mo_extensions/front/tf/
        cosh_ext.py
    Model Optimizer operation extension:
        $CLWS/cl_cosh/user_mo_extensions/ops/
        cosh.py
    Inference Engine CPU extension:
        $CLWS/cl_cosh/user_ie_extensions/cpu/
        ext_cosh.cpp
        CMakeLists.txt
    Inference Engine GPU extension:
        $CLWS/cl_cosh/user_ie_extensions/gpu/
        cosh_kernel.cl
        cosh_kernel.xml

For reference, or to copy to make the changes quicker, pre-edited template files
 are provided by the tutorial in the $CLT directory.


    Basics of the Model Optimizer
    Different Optimization Techniques and their impact on model performance
    Supported Frameworks in the Intel® Distribution of OpenVINO™ Toolkit
    Converting from models in those frameworks to Intermediate Representations
    And a bit on Custom Layers.
    
Lesson 3 Glossary:

Model Optimizer --------------
A command-line tool used for converting a model from one of the supported frameworks to an Intermediate
 Representation (IR), including certain performance optimizations, that is compatible 
 with the Inference Engine. 
 
 Optimization Techniques --------------
Optimization techniques adjust the original trained model in order to either reduce the size of 
or increase the speed of a model in performing inference. Techniques discussed in the lesson include
 quantization, freezing and fusion.
 
Quantization --------------
Reduces precision of weights and biases (to lower precision floating point values or integers),
 thereby reducing compute time and size with some (often minimal) loss of accuracy.
 
Freezing --------------
In TensorFlow this removes metadata only needed for training, as well as converting variables
 to constants. Also a term in training neural networks, where it often refers to freezing layers 
 themselves in order to fine tune only a subset of layers.
 
Fusion --------------
The process of combining certain operations together into one operation and thereby needing less
 computational overhead. For example, a batch normalization layer, activation layer, and convolutional
 layer could be combined into a single operation. This can be particularly useful for GPU inference,
 where the separate operations may occur on separate GPU kernels, while a fused operation occurs on one
 kernel, thereby incurring less overhead in switching from one kernel to the next.
 
Supported Frameworks --------------
The Intel® Distribution of OpenVINO™ Toolkit currently supports models from five frameworks 
(which themselves may support additional model frameworks): Caffe, TensorFlow, MXNet, ONNX, and Kaldi.

Caffe --------------
The “Convolutional Architecture for Fast Feature Embedding” (CAFFE) framework is an open-source deep
 learning library originally built at UC Berkeley.
 
TensorFlow --------------
TensorFlow is an open-source deep learning library originally built at Google. As an Easter egg for 
anyone who has read this far into the glossary, this was also your instructor’s first deep learning 
framework they learned, back in 2016 (pre-V1!).

MXNet --------------
Apache MXNet is an open-source deep learning library built by Apache Software Foundation.

ONNX --------------
The “Open Neural Network Exchange” (ONNX) framework is an open-source deep learning library originally
 built by Facebook and Microsoft. PyTorch and Apple-ML models are able to be converted to ONNX models.
 
Kaldi --------------
While still open-source like the other supported frameworks, Kaldi is mostly focused around speech
 recognition data, with the others being more generalized frameworks.
 
Intermediate Representation --------------
A set of files converted from one of the supported frameworks, or available as one of the 
Pre-Trained Models. This has been optimized for inference through the Inference Engine, and may 
be at one of several different precision levels. Made of two files:

    .xml - Describes the network topology
    .bin - Contains the weights and biases in a binary file

Supported Layers --------------
Layers supported for direct conversion from supported framework layers to intermediate representation
 layers through the Model Optimizer. While nearly every layer you will ever use is in the supported 
 frameworks is supported, there is sometimes a need for handling Custom Layers.
 
Custom Layers --------------
Custom layers are those outside of the list of known, supported layers, and are typically a rare
 exception. Handling custom layers in a neural network for use with the Model Optimizer depends somewhat
 on the framework used; other than adding the custom layer as an extension, you otherwise have to follow
 instructions specific to the framework.
 
 Lesson 4:
 
    Basics of the Inference Engine
    Supported Devices
    Feeding an Intermediate Representation to the Inference Engine
    Making Inference Requests
    Handling Results from the Inference Engine
    Integrating the Inference Model into an App

As you get more into working with the Inference Engine in the next exercise and into the future,
 here are a few pages of documentation I found useful in working with it.

    IE Python API: https://docs.openvinotoolkit.org/latest/ie_python_api.html
    IE Network: https://docs.openvinotoolkit.org/latest/classie__api_1_1IENetwork.html
    IE Core: https://docs.openvinotoolkit.org/latest/classie__api_1_1IECore.html
    
Further Research

    Executable Network documentation: https://docs.openvinotoolkit.org/latest/classie__api_1_1ExecutableNetwork.html
    Infer Request documentation: https://docs.openvinotoolkit.org/latest/classie__api_1_1InferRequest.html
    
Lesson 4:
	Integrating into Your App ...
	Further Research
	There’s a ton of great potential edge applications out there for you to build. Here are some
	examples to hopefully get you thinking:

    Intel®’s IoT Apps Across Industries: https://www.intel.com/content/www/us/en/internet-of-things/industry-solutions.html
    Starting Your First IoT Project: https://hackernoon.com/the-ultimate-guide-to-starting-your-first-iot-project-8b0644fbbe6d
    OpenVINO™ on a Raspberry Pi and Intel® Neural Compute Stick: https://www.pyimagesearch.com/2019/04/08/openvino-opencv-and-movidius-ncs-on-the-raspberry-pi/

Inference Engine Python API documentation reference can be found here: !!!!!!!!!!!!!!!!
	https://docs.openvinotoolkit.org/latest/_inference_engine_ie_bridges_python_docs_api_overview.html#iecore-constructor

Further Research:
The best programming language for machine learning and deep learning is still being debated,
but here’s a great blog post: https://towardsdatascience.com/what-is-the-best-programming-language-for-machine-learning-a745c156d6b7
to give you some further background on the topic.
You can check out the Optimization Guide: https://docs.openvinotoolkit.org/latest/_docs_optimization_guide_dldt_optimization_guide.html
 for more on the differences in optimization between devices.
=======
L04.9:

MQTT -----------
MQTT stands for MQ Telemetry Transport, where the MQ came from an old IBM product line called IBM MQ
for Message Queues.

Publish/Subscribe
In the publish/subscribe architecture, there is a broker, or hub, that receives messages published 
to it by different clients. The broker then routes the messages to any clients subscribing to those 
particular messages. 
MQTT is one example of this type of architecture, and is very lightweight. While you could publish 
information such as the count of bounding boxes over MQTT, you cannot publish a video frame using it. 
Publish/subscribe is also used with self-driving cars, such as with the Robot Operating System, or 
ROS for short.
 
Further Research

    Visit the main site for MQTT: http://mqtt.org/
    A helpful post on more of the basics of MQTT: https://internetofthingsagenda.techtarget.com/definition/MQTT-MQ-Telemetry-Transport

Further Research

    documentation on PyPi related to the paho-mqtt Python library: https://pypi.org/project/paho-mqtt/
    Intel® pretty neat IoT tutorial on working with MQTT with Python: https://software.intel.com/en-us/SetupGateway-MQTT

L04.11: Streaming Images to a Server
Further Research:
We covered FFMPEG [https://www.ffmpeg.org/] and ffserver, but as you may guess, there are also other ways to stream video 
to a browser. Here are a couple other options you can investigate for your own use:

    Set up Your Own Server on Linux: https://opensource.com/article/19/1/basic-live-video-streaming-server
    Use Flask and Python: https://www.pyimagesearch.com/2019/09/02/opencv-stream-video-to-web-browser-html-page/
	

In this lesson we covered:

    Basics of the Inference Engine
    Supported Devices
    Feeding an Intermediate Representation to the Inference Engine
    Making Inference Requests
    Handling Results from the Inference Engine
    Integrating the Inference Model into an App

Inference Engine
Provides a library of computer vision functions, supports calls to other computer vision libraries
 such as OpenCV, and performs optimized inference on Intermediate Representation models. Works with
 various plugins specific to different hardware to support even further optimizations.

Synchronous
Such requests wait for a given request to be fulfilled prior to continuing on to the next request.

Asynchronous
Such requests can happen simultaneously, so that the start of the next request does not need to
 wait on the completion of the previous.
 
IECore
The main Python wrapper for working with the Inference Engine. Also used to load an IENetwork,
 check the supported layers of a given network, as well as add any necessary CPU extensions.
 
IENetwork
A class to hold a model loaded from an Intermediate Representation (IR). This can then be loaded
 into an IECore and returned as an Executable Network.

ExecutableNetwork
An instance of a network loaded into an IECore and ready for inference. It is capable of both
 synchronous and asynchronous requests, and holds a tuple of InferRequest objects.

InferRequest
Individual inference requests, such as image by image, to the Inference Engine. Each of these contain
 their inputs as well as the outputs of the inference request once complete.

Lesson 5: --------------------------------

In this lesson we'll cover:

    Basics of OpenCV
    Handling Input Streams in OpenCV
    Processing Model Outputs for Additional Useful Information
    The Basics of MQTT and their use with IoT devices
    Sending statistics and video streams to a server
    Performance basics
    And finish up by thinking about additional model use cases, as well as end user needs

OpenCV has some pretty extensive tutorials available to dive deeper in:
https://docs.opencv.org/master/d9/df8/tutorial_root.html 

In this lesson we covered:

    Basics of OpenCV
    Handling Input Streams in OpenCV
    Processing Model Outputs for Additional Useful Information
    The Basics of MQTT and their use with IoT devices
    Sending statistics and video streams to a server
    Performance basics
    Thinking about additional model use cases, as well as end user needs

OpenCV
A computer vision (CV) library filled with many different computer vision functions and other useful
 image and video processing and handling capabilities.

MQTT
A publisher-subscriber protocol often used for IoT devices due to its lightweight nature. The paho-mqtt
 library is a common way of working with MQTT in Python.

Publish-Subscribe Architecture
A messaging architecture whereby it is made up of publishers, that send messages to some central broker,
 without knowing of the subscribers themselves. These messages can be posted on some given “topic”,
 which the subscribers can then listen to without having to know the publisher itself, just the “topic”.

Publisher
In a publish-subscribe architecture, the entity that is sending data to a broker on a certain “topic”.
Subscriber

In a publish-subscribe architecture, the entity that is listening to data on a certain “topic” 
from a broker.

Topic
In a publish-subscribe architecture, data is published to a given topic, and subscribers to that topic 
can then receive that data.

FFmpeg
Software that can help convert or stream audio and video. In the course, the related ffserver software
 is used to stream to a web server, which can then be queried by a Node server for viewing in a web
 browser.

Flask
A Python framework: https://www.fullstackpython.com/flask.html
 useful for web development and another potential option for video streaming to a web
 browser.

Node Server
A web server built with Node.js that can handle HTTP requests and/or serve up a webpage for viewing 
in a browser.
  
Intel® DevMesh
Check out the Intel® DevMesh website: https://devmesh.intel.com/
 for some more awesome projects others have built, join in on existing projects, or even post some 
 of your own!
    














