03/06/2019  9:47:13.60

conda create --name deep-learning python=3.6 <--- if not created
activate deep-learning
. . . .

h:\Backup\Udacity\UDLearn\deep-learning-v2-pytorch>activate deep-learning --> start command
pronpt becomes:
(deep-learning) h:\Backup\Udacity\UDLearn\deep-learning-v2-pytorch>jupyter notebook

to start notebook existed:
cd deep-learning-v2-pytorch
activate deep-learning
jupyter notebook

loss function (also called the cost): ℓ=1/2𝑛*∑𝑖𝑛(𝑦𝑖−y_hat𝑖)^2
where 𝑛 is the number of training examples, 𝑦𝑖 are the true labels, and y_hat𝑖 are the predicted labels.
=======
The sigmoid function is defined as sigmoid(x) = 1/(1+exp(-x))
4x1 + 5x2 - 9 = score; score == x to sigmoid calcs !!!!!

maximum likelihood probability P(all)
P(blue) = sigm(Wx+b)
P(red) = sigm(Wx+b)
...
P(all) = P1(blue)*P2(red)* ...*PN(...) max of maximum likelihood probability - the best separation.
cross entropy -ln(...)
connection between probabilities and error functions, and it's called Cross-Entropy. 
cross_entropy.py
import numpy as np

Probability and Cross-Entropy -------
def cross_entropy(Y, P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))
	
CE[(1,1,0)(0.8,0.7,0.9)] -> CE[Y,P] Y - 1-have present, 0 - does not have; P - probability.	

Error Function:
if y = 1 -blue point --------
P(blue) = y_hat
Error = -ln(y_hat)

if y = 0 -red point --------
P(red) = 1-P(blue) = 1 - y_hat
Error =  -ln(1 - y_hat)

Error = -(1-y)*ln(1 - y_hat) -y*ln(y_hat) 

ErrorFunc = 1/m SUM(1,m) [-(1-yi)*ln(1 - yi_hat) -yi*ln(yi_hat) ]

Gradient Descent Step:
wi′​←wi​+α(y−y^​)xi​
b′←b+α(y−y^​)

l = 1/2𝑛∑𝑖𝑛(y𝑖−y^𝑖)^2 - loss function (also called the cost)
where n is the number of training examples, y𝑖 are the true labels, and y^𝑖 are the predicted labels.

Training multilayer networks is done through backpropagation which is really just an application
 of the chain rule from calculus. 
Now that you have a trained network, you can use it for making predictions. 
This is typically called inference ---------
neural networks have a tendency to perform too well on the training data and aren't
 able to generalize to data that hasn't been seen before. This is called overfitting
 and it impairs inference performance. To test for overfitting while training, we measure
 the performance on data not in the training set called the validation set.
 
As usual, let's start by loading the dataset through torchvision.(rain=False ..!)
testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform) 

The most common method to reduce overfitting (outside of early-stopping) is dropout,
# Dropout module with 0.2 drop probability
        self.dropout = nn.Dropout(p=0.2)

During training we want to use dropout to prevent overfitting, but during inference
 we want to use the entire network. So, we need to turn off dropout during validation,
 testing, and whenever we're using the network to make predictions. To do this, you use
		model.eval()

Edit,... LaTex in Jupiter: https://gtribello.github.io/mathNET/assets/notebook-writing.html

input[channel] = (input[channel] - mean[channel]) / std[channel]
Subtracting mean centers the data around zero and dividing by std squishes the values
 to be between -1 and 1. Normalizing helps keep the network work weights near zero which
 in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn.

A list of all the available transforms:  
https://pytorch.org/docs/0.3.0/torchvision/transforms.html 

https://conda.io/projects/conda/en/latest/user-guide/getting-started.html
CONDA Commands:
Outline

    Check conda is installed and available
		$ conda -V
		conda 3.7.0
    Update conda if necessary
		conda update conda
    Create a virtual environment
		To see a list of available python versions first, type conda search "^python$"
		conda create -n yourenvname python=x.x anaconda
		conda create --name snowflakes biopython
    Activate a virtual environment
		source activate yourenvname
		conda activate snowflakes <- Windows v4.6 >
		activate snowflakes v4.6 =>
    Install additional python packages
		conda install -n yourenvname [package]
    Deactivate a virtual environment
		source deactivate
    Delete a virtual environment
		conda remove -n yourenvname -all
		
Building identical conda environments:
https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#cloning-an-environment
	conda list --explicit <-to produce a spec list
	conda list --explicit > spec-file.txt	<-To create this spec list as a file in the current working directory
To use the spec file to create an identical environment on the same machine or another machine:
	conda create --name myenv --file spec-file.txt
To use the spec file to install its listed packages into an existing environment:
	conda install --name myenv --file spec-file.txt
	
Sharing an environment
You may want to share your environment with someone else---for example,
 so they can re-create a test that you have done. To allow them to quickly 
 reproduce your environment, with all of its packages and versions, give them
 a copy of your environment.yml file.	
 Email or copy the exported environment.yml file to the other person.
Create the environment from the environment.yml file:
	conda env create -f environment.yml

Verify that the new environment was installed correctly:
	conda list

Slow internet problem:
conda config set remote_read_timeout_secs 120

 
	

	

